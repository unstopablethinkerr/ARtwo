<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hand Gesture Recognition</title>
  <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
  <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
  <style>
    /* Add the CSS from below here */

body {
  font-family: 'Roboto', sans-serif;
  margin: 1em;
  color: #3d3d3d;
  --mdc-theme-primary: #007f8b;
  --mdc-theme-on-primary: #f1f3f4;
}

h1 {
  color: #007f8b;
  text-align: center;
  font-size: 24px;
}

h2 {
  font-size: 20px;
  margin-top: 20px;
}

p {
  font-size: 14px;
  line-height: 1.5;
}

.detectOnClick {
  position: relative;
  width: 100%;
  margin: 10px 0;
  cursor: pointer;
}

.detectOnClick img {
  width: 100%;
  border-radius: 10px;
}

.videoView {
  width: 100%;
  margin: 10px 0;
}

.video-container {
  position: relative;
  width: 100%;
  height: auto;
  margin-top: 10px;
}

video, .output_canvas {
  width: 100%;
  height: auto;
  border-radius: 10px;
  transform: rotateY(180deg);
}

#gesture_output {
  display: none;
  position: absolute;
  bottom: 10px;
  left: 10px;
  background: rgba(0, 127, 139, 0.8);
  color: white;
  padding: 8px;
  border-radius: 5px;
  font-size: 14px;
}

.mdc-button {
  width: 100%;
  margin: 10px 0;
  padding: 12px;
  font-size: 16px;
}

/* Media Queries for Mobile */
@media (max-width: 600px) {
  h1 {
    font-size: 20px;
  }

  h2 {
    font-size: 18px;
  }

  p {
    font-size: 12px;
  }

  .mdc-button {
    font-size: 14px;
  }

  #gesture_output {
    font-size: 12px;
  }
} 
  </style>
</head>
<body>
  <h1>Hand Gesture Recognition</h1>

  <section id="demos" class="invisible">
    <h2>Image Recognition</h2>
    <p><em>Tap on an image below</em> to identify gestures.</p>

    <div class="detectOnClick">
      <img src="https://assets.codepen.io/9177687/idea-gcbe74dc69_1920.jpg" crossorigin="anonymous" title="Tap to recognize!" />
      <p class="classification removed"></p>
    </div>
    <div class="detectOnClick">
      <img src="https://assets.codepen.io/9177687/thumbs-up-ga409ddbd6_1.png" crossorigin="anonymous" title="Tap to recognize!" />
      <p class="classification removed"></p>
    </div>

    <h2>Live Webcam Recognition</h2>
    <p>Make gestures in front of the camera to see real-time classification. Tap <b>Enable Webcam</b> below to start.</p>

    <div id="liveView" class="videoView">
      <button id="webcamButton" class="mdc-button mdc-button--raised">
        <span class="mdc-button__ripple"></span>
        <span class="mdc-button__label">ENABLE WEBCAM</span>
      </button>
      <div class="video-container">
        <video id="webcam" autoplay playsinline></video>
        <canvas class="output_canvas" id="output_canvas"></canvas>
        <p id="gesture_output" class="output"></p>
      </div>
    </div>
  </section>

  <script type="module">
    // Add the TypeScript code from below here

import {
  GestureRecognizer,
  FilesetResolver,
  DrawingUtils
} from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

const demosSection = document.getElementById("demos");
let gestureRecognizer: GestureRecognizer;
let runningMode = "IMAGE";
let enableWebcamButton: HTMLButtonElement;
let webcamRunning: Boolean = false;

// Initialize the gesture recognizer
const createGestureRecognizer = async () => {
  const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
  );
  gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
    baseOptions: {
      modelAssetPath:
        "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
      delegate: "GPU"
    },
    runningMode: runningMode
  });
  demosSection.classList.remove("invisible");
};
createGestureRecognizer();

// Image recognition
const imageContainers = document.getElementsByClassName("detectOnClick");
for (let i = 0; i < imageContainers.length; i++) {
  imageContainers[i].children[0].addEventListener("click", handleClick);
}

async function handleClick(event) {
  if (!gestureRecognizer) {
    alert("Please wait for gestureRecognizer to load");
    return;
  }

  if (runningMode === "VIDEO") {
    runningMode = "IMAGE";
    await gestureRecognizer.setOptions({ runningMode: "IMAGE" });
  }

  const results = gestureRecognizer.recognize(event.target);

  if (results.gestures.length > 0) {
    const p = event.target.parentNode.childNodes[3];
    p.setAttribute("class", "info");

    const categoryName = results.gestures[0][0].categoryName;
    const categoryScore = parseFloat(results.gestures[0][0].score * 100).toFixed(2);
    const handedness = results.handednesses[0][0].displayName;

    p.innerText = `Gesture: ${categoryName}\nConfidence: ${categoryScore}%\nHand: ${handedness}`;
  }
}

// Webcam recognition
const video = document.getElementById("webcam");
const canvasElement = document.getElementById("output_canvas");
const canvasCtx = canvasElement.getContext("2d");
const gestureOutput = document.getElementById("gesture_output");

if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
  enableWebcamButton = document.getElementById("webcamButton");
  enableWebcamButton.addEventListener("click", enableCam);
} else {
  console.warn("getUserMedia() is not supported by your browser");
}

function enableCam(event) {
  if (!gestureRecognizer) {
    alert("Please wait for gestureRecognizer to load");
    return;
  }

  webcamRunning = !webcamRunning;
  enableWebcamButton.innerText = webcamRunning ? "DISABLE WEBCAM" : "ENABLE WEBCAM";

  if (webcamRunning) {
    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
      video.srcObject = stream;
      video.addEventListener("loadeddata", predictWebcam);
    });
  }
}

let lastVideoTime = -1;
let results = undefined;
async function predictWebcam() {
  if (runningMode === "IMAGE") {
    runningMode = "VIDEO";
    await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
  }

  let nowInMs = Date.now();
  if (video.currentTime !== lastVideoTime) {
    lastVideoTime = video.currentTime;
    results = gestureRecognizer.recognizeForVideo(video, nowInMs);
  }

  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  const drawingUtils = new DrawingUtils(canvasCtx);

  if (results.landmarks) {
    for (const landmarks of results.landmarks) {
      drawingUtils.drawConnectors(landmarks, GestureRecognizer.HAND_CONNECTIONS, {
        color: "#00FF00",
        lineWidth: 2
      });
      drawingUtils.drawLandmarks(landmarks, {
        color: "#FF0000",
        lineWidth: 1
      });
    }
  }

  if (results.gestures.length > 0) {
    gestureOutput.style.display = "block";
    const categoryName = results.gestures[0][0].categoryName;
    const categoryScore = parseFloat(results.gestures[0][0].score * 100).toFixed(2);
    const handedness = results.handednesses[0][0].displayName;
    gestureOutput.innerText = `Gesture: ${categoryName}\nConfidence: ${categoryScore}%\nHand: ${handedness}`;
  } else {
    gestureOutput.style.display = "none";
  }

  canvasCtx.restore();
  if (webcamRunning) {
    window.requestAnimationFrame(predictWebcam);
  }
}
  </script>
</body>
</html>